import numpy as np
import math
import torch
import torch.nn as nn
import torchvision
import utils.util_cam as util_cam

from . import loss
from model.hypernet import HyperNet
from model.implicit import ImplicitFunc
from model.renderer import Renderer
from model.view_estimator import Estimator
from utils.util_run import EasyDict as edict

class Graph(nn.Module):

    def __init__(self,opt):
        super().__init__()

        # define networks
        self.estimator = Estimator(opt)
        self.hypernet = HyperNet(opt)
        self.impl_func = ImplicitFunc(opt)
        self.renderer = Renderer(opt)
        network = getattr(torchvision.models,opt.arch.enc_network)
        self.encoder = network(pretrained=opt.arch.enc_pretrained)
        self.encoder.fc = nn.Linear(self.encoder.fc.in_features,opt.arch.latent_dim_sdf+opt.arch.latent_dim_rgb)

        # define parameter and initialize
        self.shape_center = nn.Parameter(torch.empty(opt.data.num_classes, opt.arch.latent_dim_sdf))
        nn.init.kaiming_uniform_(self.shape_center, a=math.sqrt(5))

        # define loss functions
        self.loss_fns = loss.Loss(self.impl_func)
    
    def forward(self,opt,var,training=False,get_loss=True,render_rnd=False):
        
        # forward the encoder and reconstructor to get the occ and rgb volume
        var.latent_raw = self.encoder(var.rgb_input_map)
        var.latent_sdf = var.latent_raw[:,:opt.arch.latent_dim_sdf]
        var.latent_rgb = var.latent_raw[:,opt.arch.latent_dim_sdf:]
        var.impl_weights = self.hypernet(opt,var.latent_sdf,var.latent_rgb)

        # predict the viewpoint
        var.pose = self.pred_pose(opt,var)

        # [B,HW,3], [B,HW,1], [B,HW,1], [B,HW,1] (mask generated by SDF of last step, not bisection; if SDF>0 then mask is 1), [B,HW,steps+1]
        batch_size = len(var.idx)
        var.rgb_recon,var.depth,var.level,var.mask,var.level_all,var.soft_mask \
            = self.renderer(opt,var.impl_weights,var.pose,intr=var.intr) # [B,HW,3]
        var.rgb_recon_map = var.rgb_recon.view(batch_size,opt.H,opt.W,3).permute(0,3,1,2) # [B,3,H,W]
        var.depth_map = var.depth.view(batch_size,opt.H,opt.W,1).permute(0,3,1,2) # [B,1,H,W]
        var.level_map = var.level.view(batch_size,opt.H,opt.W,1).permute(0,3,1,2) # [B,1,H,W]
        var.mask_map = var.mask.view(batch_size,opt.H,opt.W,1).permute(0,3,1,2) # [B,1,H,W]
        var.soft_mask_map = var.soft_mask.view(batch_size,opt.H,opt.W,1).permute(0,3,1,2) # [B,1,H,W]
        var.fake_recon = torch.cat([var.soft_mask_map, var.rgb_recon_map], dim=1)
        var.real_input = torch.cat([var.mask_input_map, var.rgb_input_map], dim=1)
        if render_rnd: 
            var = self.render_random_view(opt,var)
        if get_loss: 
            loss = self.compute_loss(opt,var,training)
            return var, loss
        return var

    def render_random_view(self,opt,var):
        batch_size = len(var.idx)
        var.rnd_pose = self.sample_pose(opt,var)
        # inference the implicit function with lstm to get the rgb image
        rgb_recon,_,_,mask,_,soft_mask = self.renderer(opt,var.impl_weights,var.rnd_pose,intr=var.intr) # [B,HW,3]
        var.rgb_rnd_view = rgb_recon.view(batch_size,opt.H,opt.W,3).permute(0,3,1,2) # [B,3,H,W]
        var.mask_rnd_view = mask.view(batch_size,opt.H,opt.W,1).permute(0,3,1,2) # [B,1,H,W]
        var.soft_rnd_view = soft_mask.view(batch_size,opt.H,opt.W,1).permute(0,3,1,2) # [B,1,H,W]
        var.fake_rnd_view = torch.cat([var.soft_rnd_view, var.rgb_rnd_view], dim=1)
        return var

    def compute_loss(self,opt,var,training=False):
        loss = edict()
        # IOU loss
        if opt.loss_weight.mask_iou is not None:
            loss.mask_iou = self.loss_fns.iou_loss(var.soft_mask_map,var.mask_input)
        # SDF-SRN losses
        if opt.loss_weight.render is not None:
            loss.render = self.loss_fns.MSE_loss(var.rgb_recon,var.rgb_input)
        if opt.loss_weight.shape_silh is not None:
            loss.shape_silh = self.loss_fns.shape_from_silhouette_loss(opt,var)
        if opt.loss_weight.ray_intsc is not None:
            loss.ray_intsc = self.loss_fns.ray_intersection_loss(opt,var)
        if opt.loss_weight.ray_free is not None:
            loss.ray_free = self.loss_fns.ray_freespace_loss(opt,var)
        if opt.loss_weight.eikonal is not None:
            var.sdf_grad_norm = self.sdf_gradient_norm(opt,var.impl_weights,batch_size=len(var.idx))
            loss.eikonal = self.loss_fns.MSE_loss(var.sdf_grad_norm,1)
        # categorical metric regularization
        if opt.loss_weight.cat_metric is not None and training:
            loss.cat_metric = self.loss_fns.category_metric_loss(opt,var,self.shape_center)
        # view consistency loss
        if opt.loss_weight.view_consistency is not None and training:
            loss.view_consistency = self.loss_fns.view_consistency_loss(opt,var,self.estimator)
        return loss

    def azim2mat_shapenet13(self,opt,trig_azim=None,angle_azim=None):
        batch_size = trig_azim.shape[0] if angle_azim is None else angle_azim.shape[0]
        angle_x = torch.ones(batch_size, 1).to(opt.device) * (-np.pi/6)
        angle_z = torch.ones(batch_size, 1).to(opt.device) * np.pi
        if angle_azim is not None:
            angle_y = angle_azim
            angles = torch.cat([angle_x, angle_y, angle_z], dim=1)
            rotmat = util_cam.euler2mat(angles)
        else:
            angle_y = torch.zeros(batch_size, 1).to(opt.device)
            angles = torch.cat([angle_x, angle_y, angle_z], dim=1)
            rotmat = util_cam.euler2mat(angles, trig_y=trig_azim)
        translations = torch.tensor([0.,0.,opt.camera.dist]).to(opt.device).view(1,3,1).expand(batch_size,3,1)
        pose = torch.cat([rotmat, translations], dim=-1)
        return pose

    def pred_pose(self,opt,var):
        img_viewpoint = torch.cat([var.rgb_input_map, var.mask_input_map], dim=1)
        var.trig_azim = self.estimator(img_viewpoint)
        pose = self.azim2mat_shapenet13(opt,var.trig_azim,angle_azim=None)
        return pose

    def sample_pose(self,opt,var):
        batch_size = len(var.idx)
        device = var.rgb_input.device
        angle_y = torch.randint(low=0, high=24, size=(batch_size, 1)).to(device).float() / 12 * np.pi
        var.trig_azim_rnd = torch.cat([torch.cos(angle_y),torch.sin(angle_y)], dim=1)
        rnd_pose = self.azim2mat_shapenet13(opt,var.trig_azim_rnd,angle_azim=None)
        return rnd_pose

    def sdf_gradient_norm(self,opt,impl_weights,batch_size,N=10000):
        lower,upper = opt.impl.sdf_range
        points_3D = torch.rand(batch_size,N,3,device=impl_weights.level[0][0].device)
        points_3D = points_3D*(upper-lower)+lower
        with torch.enable_grad():
            points_3D.requires_grad_(True)
            level = self.impl_func(opt,points_3D,impl_weights) # [B,HW,1]
            grad = torch.autograd.grad(level.sum(),points_3D,create_graph=True)
        grad_norm = grad[0].norm(dim=-1,keepdim=True)
        return grad_norm

